= Red Hat Advanced Cluster Management for Kubernetes

In this session, participants will learn how to use Red Hat Advanced Cluster Management for Kubernetes (RHACM). The session will cover three major use cases:

*Multi-Cluster Lifecycle*

* Centrally manage, create, update, and delete Kubernetes clusters
* Search, find, and modify Kubernetes resources across the entire domain
* Troubleshoot and resolve issues across your federated domain quickly

*Virtual Machine Lifecycle*

* Centrally manage, create, update, and delete Virtual Machines
* Search, find, and modify VMs across the entire domain managed by RHACM
* Troubleshoot and resolve issues with observability across your VMs

*Advanced Application Lifecycle*

* Deploy applications at scale using GitOps
* Monitor applications in real time from multiple sources
* Visualize application relationships across clusters, including multi-cluster deployments

Lets get started!

[[cluster-lifecycle]]

=== Working with Kubernetes Clusters and Cluster Lifecycle

At a high level, Cluster Lifecycle Management involves creating, upgrading, destroying, and importing clusters in a multi-cloud environment. This includes traditional OpenShift clusters, managed environments like ROSA or ARO, and specialized deployments such as Hosted Control Planes or Single-Node OpenShift.

[[hosted-control-planes]]

=== Working with Hosted Control Planes

Hosted Control Planes is a variant of Red Hat OpenShift that follows a distinct architectural model. In standalone OpenShift, the control and data planes are co-located, with a dedicated set of nodes hosting the control plane to ensure quorum, and the network stack is shared. While functional, this setup may not be ideal for large-scale, multi-cluster deployments.

To address these challenges, Red Hat offers Hosted Control Planes, based on the upstream HyperShift project. This model acts as middleware for hosting OpenShift control planes at scale, providing benefits like reduced costs, faster provisioning, and a clear separation between management and workloads.

Hosted Control Planes for Red Hat OpenShift decouple the control plane from the data plane, offering several key advantages:

Network Separation: Provides distinct network domains for the control plane and workloads.

Centralized Management: Fleet administrators and Site Reliability Engineers (SREs) can manage multiple clusters through a shared interface.

Unified Monitoring and Security: Administrators treat the control plane like any other workload, using the same tools to monitor, secure, and operate both the applications and control plane.

Key Benefits of Decoupling the Control and Data Planes:

* Trust Segmentation & Human Error Reduction: Separates management planes and cloud credentials from end-user clusters, reducing the risk of mistakes that could compromise the control plane.
* Cost Efficiency: You can host 7-21 control planes on the same hardware that would typically host just one, leading to cheaper High Availability (HA) clusters.
* Fast Cluster Provisioning: Control plane pods are launched on OpenShift, eliminating the need to wait for hardware provisioning.
* Kubernetes Managing Kubernetes: Treating the control plane as Kubernetes workloads unlocks features like Horizontal Pod Autoscaling (HPA), Vertical Pod Autoscaling (VPA), and control-plane hibernation.
* Component Isolation: Infrastructure components like registries and storage nodes can be isolated to the tenant's cloud provider account.
* Lifecycle Flexibility: You can upgrade control planes separately from worker nodes, even for CVEs under embargo.
* Future Management Flexibility: The architecture allows for the potential of running the control plane on a different Infrastructure-as-a-Service (IaaS) provider than the workers.
* Heterogeneous Architecture: Easier to run control planes on one CPU type (e.g., x86) and workers on another (e.g., ARM or Power/Z).
* Easier Multi-Cluster Management: Centralized multi-cluster management improves consistency and reduces external factors affecting cluster health.
* Cross-Cluster Delivery: Externalized control planes simplify delivering cross-cluster solutions like service mesh, serverless, and pipelines.
* Improved Operability: SREs benefit from a centralized interface to manage and troubleshoot control planes, leading to faster issue resolution and increased productivity.

==  RHACM console access

The RHACM console is available in the OpenShift cluster console at: {openshift_console_url}

Administrator login is available with:

[cols="1,1"]
|===
| *Username:* | kubeadmin 
| *Password:* | {openshift_kubeadmin_password}
|===

Navigate to the *Cluster* drop down menu and then select *All Clusters*. 

image::101-local-cluster.png[link=self, window=blank, width=100%, Cluster Selection Menu]

In this interface you will see 2 clusters available, the first cluster is a Hosted Control Plane Cluster called *"development"*. The second cluster is our working cluster labeled *"local-cluster"*.

image::102-cluster-view.png[link=self, window=blank, width=100%, View of Clusters Listed]


[[create-manage-cluster]]

== Create and Manage Kubernetes Clusters

Red Hat Advanced Cluster Management for Kubernetes (RHACM) simplifies the deployment and management of additional clusters. While Red Hat OpenShift offers easy deployment methods like IPI and the Assisted Installer, RHACM takes it a step further, allowing you to deploy new clusters with just a few clicks using the cluster creation wizard.

From the Clusters screen, you can quickly see how simple it is to deploy a new cluster.

*Procedure*

[start=1]
. Click on the *Create cluster* button in the center of the screen:

image::103-create-cluster.png[link=self, window=blank, width=100%, Create Cluster]

NOTE: You’ll notice that the AWS option is already highlighted, indicating that your credentials are saved. You will use this to deploy the new cluster, but feel free to explore the window to see other available cluster types.

[start=2]
. Click on the AWS button.

image::104-aws-credentials.png[link=self, window=blank, width=100%, AWS Credentials]

You will see two options for the control plane type: 

* Hosted Control Plane
* Standalone

The *development* cluster that you have provisioned is an example of a Hosted Control Plane cluster, which you explained the benefits of in detail in the link:module-03.html#hosted-control-planes[Introducing Hosted Control Planes] section above. 

For our lab, you will be using the *Standalone* cluster option. 

[start=3]
. Click on the *standalone* option.
. Name your cluster *demo-cluster*, and select *default* for the cluster set. 
. Next, select the most recent release image *OpenShift 4.18.8*. (Or whatever is the most recent option, it does not matter)
. Click on *Next* to continue.

image::105-create-cluster-details.png[link=self, window=blank, width=100%, Create Cluster Details]

On the next screen You can customize the AWS region, the CPU architecture, and the number of nodes to deploy in the control plane and worker pools. 

[start=7]
. Click on *Next* to proceed.

image::106-create-cluster-nodepools.png[link=self, window=blank, width=100%, Create Cluster NodePools]

The next screen allows you to configure networking type to use and it's associated variables. 

[start=8]
. Click on *Next* to proceed.

image::107-create-cluster-networking.png[link=self, window=blank, width=100%, Create Cluster Networking]

The next couple of screens allow for additional customization, configuring a proxy, creating private AWS configurations, and pre-configuring automation functions with Ansible Automation Platform. 

[start=9]
. Click *Next* on each screen to proceed to the final *Review and Create* screen.

You will see a description of the cluster you are creating.

[start=10]
. Click the blue *Create* button to start the deployment process.

image::108-create-cluster-summary-create.png[link=self, window=blank, width=100%, Create Cluster Summary Page]

[start=11]
. Let the UI do it's thing. You should see the Cluster creation process starting in the UI.

image::03-cluster-creation.png[link=self, window=blank, width=100%, View New Cluster]

NOTE: Deploying a full cluster will take about 45 minutes. The main goal of this part of the lab is to show how easy it is to deploy clusters. You’ll continue the lab using the infrastructure that's already set up.

[[create-manage-vms]]

== Create and Manage Virtual Machines

Do you want to manage and provision OpenShift Virtualization virtual machine workloads across multiple clusters using a single source of truth in the GitOps way? In this exercise, you'll learn how to do that with Red Hat Advanced Cluster Management (RHACM) and OpenShift GitOps.

This process uses OpenShift Virtualization, which leverages KubeVirt, an open-source project that allows you to run, deploy, and manage virtual machines (VMs) with Kubernetes as the orchestration platform. This approach, known as container-native virtualization, packages VMs inside containers, enabling you to manage both VMs and container workloads from a single RHACM Console.

=== How does OpenShift Virtualization work?

OpenShift Virtualization uses KubeVirt to extend the Kubernetes API, enabling it to interact with virtual machines just like other Kubernetes resources. This allows containers and virtual machines to share the same cluster, nodes, and networks.

*OpenShift Virtualization added functionality is composed of 3 main components:*

* *Custom Resource Definitions (CRDs)*: A custom resource extends the Kubernetes API, allowing you to introduce your own API to a project or cluster. KubeVirt adds a CRD to the Kubernetes API, enabling it to manage virtual machines like other Kubernetes objects (e.g., pods).

* *Controllers:* Controllers are sets of deployments running on the cluster that provide API endpoints for managing the new KubeVirt CRDs.

* *Agents:* Agents run on worker nodes in the cluster, managing node tasks related to virtualization.

Think of OpenShift Virtualization as a pod running a KVM-based virtual machine. In Kubernetes, a pod is a group of containers sharing resources. KVM (Kernel-based Virtual Machine) is an open-source technology that turns the Linux kernel into a hypervisor. With KubeVirt, VM instances run like pods, allowing OpenShift Virtualization to manage VM states (e.g., "stopped," "paused," "running") and perform operations like provisioning, scheduling, and migrating virtual machines.

=== Deploy a Virtual Machines Using OpenShift GitOps

Red Hat® OpenShift® GitOps is an operator that streamlines workflows by integrating git repositories, CI/CD tools, and Kubernetes. This enables faster, more secure, and scalable software development while maintaining quality.

OpenShift GitOps builds declarative, Git-driven CD workflows directly into the application development platform. It automates infrastructure and deployment requirements by pushing updates and changes through declarative code.

OpenShift® GitOps uses Argo CD, integrated with Red Hat Advanced Cluster Management for Kubernetes (RHACM), to provide a consistent and fully supported Kubernetes platform for GitOps principles.

With RHACM, users can enable the optional Argo CD pull model architecture, which is ideal for scenarios where the centralized cluster cannot reach remote clusters, but the remote clusters can communicate with the centralized one. In these cases, the pull model is more feasible than the push model.

Argo CD typically uses a push model where the workload is pushed from a centralized cluster to remote clusters. The pull model, however, allows the Argo CD Application CR to be distributed to remote clusters, where each cluster independently reconciles and deploys the application. The application status is reported back to the centralized cluster, mimicking the push model UX.

The pull model offers decentralized control, where each cluster manages its own configuration and independently pulls updates. This reduces the need for centralized management, making the system more scalable and easier to manage. However, the hub cluster can still be a single point of failure, so redundancy should be considered.

The pull model also provides more flexibility, allowing clusters to pull updates on their own schedule, which helps avoid conflicts or disruptions.

For this exercise, you will use the Push Model.

NOTE: ArgoCD has been deployed in your enviroment however you will need to configure it in RHACM.

=== Integrate ArgoCD with RHACM

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application, select ArgoCD AppicationSet-Push Model*.
. Under the Argo server select *Add Argo Server* 
. Enter the following information:
* *Name:* openshift-gitops
* *Namespace:* openshift-gitops
* *ClusterSet:* default

image::03-argoconfig.png[link=self, window=blank, width=100%, ArgoCD Config]

Perfect! Next you will use ArgoCD to deploy a Virtual Machine

=== Deploy an Virtual Machine 


*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application, select ArgoCD AppicationSet-Push Model*.
. Enter the following information:
* *Name:* dev-vm
* *Namespace:* openshift-gitops
* Click *NEXT*

image::03-vm-app-acm.png[link=self, window=blank, width=100%, VM Config]

[start=4]
. Under repository types, select the GIT repository
- *URL:* https://github.com/jalvarez-rh/kubevirt-gitops
- *Revision:* main
- *Path:* vms
- *Destination:* openshift-cnv
. Click *NEXT TWICE*

image::03-vm-app-git.png[link=self, window=blank, width=100%, VM ACM Config]


[start=6]
. Under *Placement* for application deployment, verify that *New Placement* is selected.
- *Cluster set:* default
. Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any of
* *Values:* local-cluster


image::03-vm-placement-acm.png[link=self, window=blank, width=100%, VM ACM Config 2]

[start=8]
. Click *NEXT - verify that all the information is correct.*
. Click *Submit* 

It will take a few minutes to deploy the application, click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-acm-vm-topology.png[link=self, window=blank, width=100%, Application Topology]

[start=10]
. Go to *Infrastructure* then select *Virtual Machines* 

In this tab you will see a list of the available virtual machines, if you completed all of the steps above you should see a VM labeled *rhel9-gitops*.,

image::03-vm-acm-view.png[link=self, window=blank, width=100%, VM View]

From this point you can interact with the virtual machine directly from ACM.

[start=11]
. Click the *Launch* button to see all of the information about the Virtual Machine.

image::03-vm-actions.png[link=self, window=blank, width=100%, VM View]

Congratulations! You have successfully deployed a Virtual Machine using Red Hat GitOps. This approach utilized a Git repository containing all the manifests defining your VMs. RHACM used those manifests as deployables, which were then deployed to the target cluster, enabling easy management of your resources.

[[policy-driven-governance]]

== Deploy Applications Using OpenShift GitOps

You've deployed your clusters and VMs, and now it's time to create some containerized applications, starting with the Python application from the Quay module.

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application*, select *ArgoCD AppicationSet-Push Model*.
. Enter the following information:
* *Name:* skupper-patient-demo
* *Argo Server:* openshift-gitops
. Click *NEXT*

image::03-app-gitops.png[link=self, window=blank, width=100%, App GitOps]

[start=5]
. Under repository types, select the GIT repository
. enter the URL: https://github.com/mfosterrox/skupper-security-demo.git
. Set Revision: main
. Set Path: skupper-demo
. Set Destination: patient-portal
. Then click *NEXT TWICE*

image::03-app-gitops-2.png[link=self, window=blank, width=100%, App ACM GitOps]

[start=11]
. Under *Sync Policy* uncheck *Automaticaly sync when cluster state changes* and check *Replace resources instead of applying changes from the source repository* 

image::03-app-gitops-3.png[link=self, window=blank, width=100%, App ACM GitOps]

NOTE: These changes are only required as you will be modifying the application YAML on RHACM and you don't want it to sync to a Git Repo, you normaly wouldn't uncheck these in a real production enviroment.

[start=12]
. Under *Placement* for application deployment, verify that *New Placement* is selected.
* Cluster set: default
. Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any off
* *Values:* local-cluster

image::03-app-placement.png[link=self, window=blank, width=100%, ACM App Placement]

[start=14]
. Verify all of the information is correct and click *Submit*.

NOTE: It will take a few minutes to deploy the application

[start=15]
. Click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-application-topology-git.png[link=self, window=blank, width=100%, Application Topology]

[start=16]
. Under the topology view, Select the *Route* and click on the *Launch Route URL*

NOTE: This will take you to the Front end for the Patient Portal application, which is now running in our Hosted Controlled Plane (HCP) CLuster.

IMPORTANT: If you get a "Application is not available" page change the URL to use http:// and that should fix the issue. 

image::03-application-route-git.png[link=self, window=blank, width=100%, Application Route]

Congratulations! 

You have successfully deployed an application using RHACM and OpenShift GitOps. This approach utilized a Git repository containing the manifests that defined your application. RHACM took those manifests and used them as deployables, which were then deployed to the target cluster.

[[updating-an-application]]

=== Updating the Frontend Application

Let's now use your local Quay repo that contains the frontend image you built earlier. This image includes a few key security issues that you will explore in more detail during the ACS module later on.

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click on *Filter* and under *type* select *Application Set* 
. Click on the *patient-demo* App and Click *Topology*
. Click on *deployment* then find *frontend* Click on *frontend*

image::03-deployment-patient.png[link=self, window=blank, width=100%, Application Deployment]

[start=5]
. Click *Launch resource in Search* a new window will pop up

image::03-frontend-search.png[link=self, window=blank, width=100%, Application Frontend]

[start=6]
. Under Deployment find *frontend* and click it

image::03-frontend-search2.png[link=self, window=blank, width=100%, Application Frontend Search]

[start=7]
. Click on the *YAML* tab and under spec:containers find the *image* field

image::03-frontend-search3.png[link=self, window=blank, width=100%, Application Frontend Search Image]

[start=8]
. Navigate back the Terminal screen and execute the following command

[source,sh,subs="attributes",role=execute]
----
echo $QUAY_URL/$QUAY_USER/frontend:0.1
----

[start=9]
. Copy the Quay repo URL below

[source,sh,subs="attributes",role=execute]
----
$QUAY_URL/$QUAY_USER/frontend:0.1
----

[start=10]
. Navigate back to Search and replace the *image* with the URL you just copy from the terminal. Click *Save*

image::03-frontend-image.png[link=self, window=blank, width=100%, Application Frontend Image New]

[start=11]
. Navigate back to ACM / Application / Topology View 
. Click on Deployment / Frontend. Verify that the image url has changed

image::03-frontend-image2.png[link=self, window=blank, width=100%, Application Frontend Image Toplogy]

IMPORTANT: The Pod in the Topology View should turn RED. This is expected because you're making changes on the ACM side but not on the GitOps side, causing a reporting difference. In a real production environment, you wouldn't make such changes manually, but this is done for educational purposes only.

[[acm-conclusion]]

== Conclusion

Congratulations! You have successfully deployed an application to a Kubernetes cluster using RHACM. This approach utilized a Git repository containing the manifests that defined your application. RHACM took those manifests and used them as deployables, which were then deployed to the target cluster.

In the next module, you'll dive deeper into the security aspects of the frontend image you just deployed. You will scan it, explore vulnerabilities, and much more.